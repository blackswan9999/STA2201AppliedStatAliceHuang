---
title: "STA2201 ASSIGNMENT 2 Alice Huang"
author: "Alice Huang"
date: "05/03/2023"
output: pdf_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
# for bayes stuff
library(here)
library(rstan)
library(bayesplot) 
library(loo) 
library(tidybayes) 
```


# Question 1

## Question 1a

We know that if $\mu \sim N(\mu_0 , \sigma^2_{\mu_0})$ and $y_{i}|\mu, \sigma^2 \sim N(\mu, \sigma^2 )$ and $\sigma^2$ is known, then

$$\mu|y,\sigma^2 \sim N(\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}, \frac{1}{1/\sigma^2_{\mu_0} + n/\sigma^2})$$

We are given that $$\mu_0 = 100, \sigma_{\mu_0} = 15, n=10, \sigma = 15, \bar{y} = 113$$

```{r}
mu0 <- 100
sigma_mu0 <- 15
n <- 10
sigma <- 15
ybar <- 113

post_mean <- ((mu0/(sigma_mu0^2)) + (n*ybar/(sigma^2)))/((1/(sigma_mu0^2)) + (n/(sigma^2))) 

post_var <- 1/((1/(sigma_mu0^2))+(n/(sigma^2)))
```


So 

$$\mu|y,\sigma^2 \sim N(\frac{100/15^2 + 10(113)/15^2}{1/15^2 + 10/15^2}, \frac{1}{1/15^2 + 10/15^2}) = N(\frac{1230}{11},\frac{15^2}{11}) = N(111.8182, 20.45455)$$

The Bayesian point estimate is given by the posterior mean $\hat{\mu} = E(\mu|y) = \frac{1230}{11} = 111.8182$.

The 95% credible interval is given by $(\hat{\mu} - q_{0.975}\sigma, \hat{\mu} + q_{0.975}\sigma)$

```{r}
c(post_mean - qnorm(0.975)*sqrt(post_var), post_mean + qnorm(0.975)*sqrt(post_var))
```

$(111.8182 - 1.96\sqrt{20.45455} , 111.8182 + 1.96\sqrt{20.45455}) = (102.9539,120.6825)$

So the 95% credible interval is $(102.9539,120.6825)$.

## Question 1b

$E((\hat{\mu}-\mu^*)^2 | \mu^* ) = E((\hat{\mu}-E(\hat{\mu}|\mu^*)+E(\hat{\mu}|\mu^*)-\mu^*)^2 | \mu^* )$

$= E((\hat{\mu}-E(\hat{\mu}|\mu^*))^2|\mu^*)+ 2E((\hat{\mu}-E(\hat{\mu}|\mu^*))(E(\hat{\mu}|\mu^*)-\mu^*)|\mu^*) + E((E(\hat{\mu}|\mu^*)-\mu^*)^2 | \mu^* )$

$= Var(\hat{\mu}|\mu^* ) + 2 E[\hat{\mu} E(\hat{\mu} | \mu^*) - \hat{\mu}\mu^* - E(\hat{\mu}|\mu^*)^2 + \mu^* E(\hat{\mu}|\mu^*)|\mu^*] + E[E(\hat{\mu}|\mu^*)^2 - 2\mu^* E(\hat{\mu}|\mu^*) + \mu^*|\mu^*]$

$= Var(\hat{\mu}|\mu^* ) + E[2\hat{\mu} E(\hat{\mu} | \mu^*) - 2\hat{\mu}\mu^* - 2E(\hat{\mu}|\mu^*)^2 + 2\mu^* E(\hat{\mu}|\mu^*)+ E(\hat{\mu}|\mu^*)^2 - 2\mu^* E(\hat{\mu}|\mu^*) + \mu^*|\mu^*]$

$= Var(\hat{\mu}|\mu^* ) + 2E(\hat{\mu}|\mu^*) E(\hat{\mu} | \mu^*) - 2E(\hat{\mu}|\mu^*)\mu^* - E(\hat{\mu}|\mu^*)^2 + \mu^*$

$= Var(\hat{\mu}|\mu^* ) + E(\hat{\mu}|\mu^*)^2 - 2E(\hat{\mu}|\mu^*)\mu^* + \mu^*$

$= Var(\hat{\mu}|\mu^* ) + (Bias(\hat{\mu}|\mu^*))^2$

## Question 1c

We derive the distribution of $\hat{\mu}_{BAYES} = \frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}$

Since $y_i | \mu, \sigma^2 \sim N(112, 15^2 )$ we have that 

$E[\hat{\mu}_{BAYES}] = E\bigg[\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}\bigg] = \frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}$

$Var[\hat{\mu}_{BAYES}] = Var\bigg[\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}\bigg] = \frac{(n/\sigma^2 )^2 (\sigma^2 /n)}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2} = \frac{n/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2}$

So $\hat{\mu}_{Bayes} \sim N(\frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}, \frac{n/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2})$

Then we get 

$$\hat{\mu}_{Bayes} \sim N(\frac{100/(15^2) + 10(112)/15^2}{1/15^2 + 10/15^2}, \frac{10/15^2}{(1/15^2 + 10/15^2)^2}) \sim N(110.9091,18.59504)$$

So 

$$MSE(\hat{\mu}_{Bayes}|\mu^* = 112) = Var(\hat{\mu}|\mu^* ) + (Bias(\hat{\mu}|\mu^*))^2 = 18.59504 + (110.9091 - 112)^2 = 19.7851$$

## Question 1d

We have $\mu^* = 112$. The ML estimator is the sample mean, which is $\bar{y}$. $y \sim N(112, 15^2) \implies \bar{y} \sim N(112, \frac{15^2}{10})$.

Now we derive the distribution of $\hat{\mu}_{BAYES} = \frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}$

Since $y_i | \mu, \sigma^2 \sim N(112, 15^2 )$ we have that 

$E[\hat{\mu}_{BAYES}] = E\bigg[\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}\bigg] = \frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}$

$Var[\hat{\mu}_{BAYES}] = Var\bigg[\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}\bigg] = \frac{(n\mu/\sigma^2 )^2 (\sigma^2 /n)}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2} = \frac{n\mu/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2}$

So $\hat{\mu}_{Bayes} \sim N(\frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}, \frac{n\mu/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2})$

Then the sample mean of $\hat{\mu}_{Bayes}$, $\bar{\hat{\mu}}_{Bayes}$ follows the following distribution:
$$N(\frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}, \frac{1/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2})$$

We substitute $$\mu_0 = 100, \sigma_{\mu_0} = 15, n=10, \sigma = 15, \mu = 112$$

```{r}
mu0 <- 100
sigma_mu0 <- 15
n <- 10
sigma <- 15
mu <- 112

mu_hat_bayes_bar_mean <- function(mu0, sigma_mu0, n, sigma, mu){
  ((mu0/(sigma_mu0^2)) + (n*mu/(sigma^2)))/((1/(sigma_mu0^2)) + (n/(sigma^2))) }
mu_hat_bayes_bar_mean(mu0, sigma_mu0, n, sigma, mu)

mu_hat_bayes_bar_var <- function(mu0, sigma_mu0, n, sigma, mu){
  (1/(sigma^2))/(((1/(sigma_mu0^2))+(n/(sigma^2)))^2) }
mu_hat_bayes_bar_var(mu0, sigma_mu0, n, sigma, mu)
```

Then we get 

$$\bar{\hat{\mu}}_{Bayes} \sim N(\frac{100/(15^2) + 10(112)/15^2}{1/15^2 + 10/15^2}, \frac{1/15^2}{(1/15^2 + 10/15^2)^2}) \sim N(110.9091,1.859504)$$

```{r}
df = data.frame(x=seq(95, 130, 0.5), 
                ML_sampling_distribution = dnorm(x=seq(95, 130, 0.5) , mean=112, sd = 15/sqrt(10)),
                Bayes_estimator_sampling_distribution = dnorm(x=seq(95, 130, 0.5), mean=110.9091, sd = sqrt(1.859504)))

ggplot(df, aes(x=x, y=ML_sampling_distribution)) + geom_line() + stat_function(fun=dnorm, args = list(110.9091, 1.859504), col="red")
```

$MSE(\bar{y}|\mu) = Var(\bar{y}|\mu) + [E(\bar{y}|\mu)-\mu]^2 = \frac{\sigma^2}{n}$

$MSE(\hat{\mu}_{Bayes}) = Var(\bar{y}|\mu) + [E(\hat{\mu}_{Bayes}|\mu)-\mu]^2$

```{r}
MSE_Bayes_estimate <- function(mu0, sigma_mu0, n, sigma, mu){
  r = n*mu_hat_bayes_bar_var(mu0, sigma_mu0, n, sigma, mu) + (mu_hat_bayes_bar_mean(mu0, sigma_mu0, n, sigma, mu) - mu)^2
  return(r)
}

MSE_ML <- function(sigma, n){
  result = (sigma^2)/n
  return(result)
}


MSE_ratio <- function(mu0, sigma_mu0, n, sigma, mu){
  result = MSE_Bayes_estimate(mu0, sigma_mu0, n, sigma, mu)/MSE_ML(sigma, n)
  return(result)
}

n = seq(0, 20)
  
plot(n, MSE_ratio(mu0, sigma_mu0, n, sigma, mu), ylab="MSE ratio")
```

We can see that as sample size increases, the ratio of the MSE of the Bayes estimate to the MSE of the ML estimate increases.


# Question 2

```{r include=FALSE}
sweden <- read_csv("sweden.csv")
```
## Question 2a

```{r}
sweden %>% ggplot(aes(x=age, y=deaths, color = year)) + geom_point()
```

Over time, the number of deaths for people ages 50-80 has decreased but the number of deaths for people ages 80-100 has risen.

```{r}
sweden %>% ggplot(aes(x=age, y=deaths/pop, color = year)) + geom_point()
```


## Question 2b




# Question 3

```{r, echo=FALSE, warning = FALSE, message=FALSE}
wells <- read.table("http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat", sep=" ")
```

## Question 3a

```{r}
wells %>% ggplot(aes(x=dist, col=factor(switch))) + geom_boxplot()
```

```{r}
wells %>% ggplot(aes(x=arsenic, col=factor(switch))) + geom_boxplot()
```

```{r}
wells %>% ggplot(aes(x=dist, y=arsenic, col=factor(switch))) + geom_point() + geom_smooth() +theme_bw()
```

## Question 3b

```{r}
wells %>% mutate(cent_dist = dist - mean(dist), cent_arsenic = arsenic - mean(arsenic)) %>%
  mutate(cent_dist_cent_arsenic = cent_dist*cent_arsenic) -> wells
```

```{r}
stan_data <- list(N = nrow(wells),
                  switch_well = wells$switch,
                  cent_dist = wells$cent_dist,
                  cent_arsenic = wells$cent_arsenic,
                  cent_dist_cent_arsenic = wells$cent_dist_cent_arsenic)

wells1 <- stan(data = stan_data, 
             file = here("wells.stan"),
             iter = 500,
             seed = 0)
```

```{r}
wells %>% mutate(cent_log_arsenic = log(arsenic) - mean(log(arsenic))) %>%
  mutate(cent_dist_cent_log_arsenic = cent_dist*cent_log_arsenic) -> wells
```

```{r}
stan_data2 <- list(N = nrow(wells),
                  switch_well = wells$switch,
                  cent_dist = wells$cent_dist,
                  cent_log_arsenic = wells$cent_log_arsenic,
                  cent_dist_cent_log_arsenic = wells$cent_dist_cent_log_arsenic)

wells2 <- stan(data = stan_data2, 
             file = here("wells2.stan"),
             iter = 500,
             seed = 0)
```

```{r}
summary(wells1)$summary[c("beta[1]", "beta[2]", "beta[3]", "beta[4]"),]
```

For every unit increase in distance, the odds of switching a well are multiplied by exp(-0.008793405), assuming all other factors are held constant.

For every unit increase in arsenic levels, the odds of switching a well are multiplied by exp(0.467958876), assuming all other factors are held constant.

For every unit increase in demeaned distance multiplied by demeaned arsenic levels, the odds of switching a well are multiplied by exp(-0.001760422), assuming all other factors are held constant.

## Question 3c

```{r}
t_test_stat = function(y,a){
  yind <- ifelse(y=1 & a<0.82, 1, 0)
  yind2 <- ifelse(a < 0.82, 1, 0)
  num = sum(yind)
  denom = sum(yind2)
  return(num/denom)
}
```

```{r}
yrep1 <- extract(wells1)[["log_switch_rep"]]
yrep2 <- extract(wells2)[["log_switch_rep"]]
cent_arsenic <- wells$cent_arsenic
```

```{r}
teststat = t_test_stat(wells$switch, cent_arsenic)
teststat_rep <- sapply(1:nrow(yrep1), function(i)t_test_stat(yrep1[i,],cent_arsenic))
teststat_rep_2 <- sapply(1:nrow(yrep2), function(i) t_test_stat(yrep2[i,],cent_arsenic))

ggplot(data = as_tibble(teststat_rep), aes(value)) + 
    geom_histogram(aes(fill = "replicated")) + 
    geom_vline(aes(xintercept = teststat, color = "observed"), lwd = 1.5) + 
  ggtitle("Model 1: test stat") + 
  scale_color_manual(name = "", values = c("observed" = "purple"))+
  scale_fill_manual(name = "", values = c("replicated" = "pink")) + theme_bw()

ggplot(data = as_tibble(teststat_rep_2), aes(value)) + 
    geom_histogram(aes(fill = "replicated")) + 
    geom_vline(aes(xintercept = teststat, color = "observed"), lwd = 1.5) + 
  ggtitle("Model 2: test stat") + 
  scale_color_manual(name = "", values = c("observed" = "purple"))+
  scale_fill_manual(name = "", values = c("replicated" = "pink")) + theme_bw()

```

## Question 3d

```{r}
library(loo)
loglik1 <- extract(wells1)[["log_lik"]]
loglik2 <- extract(wells2)[["log_lik"]]
loo1 <- loo(loglik1, save_psis = TRUE)
loo2 <- loo(loglik2, save_psis = TRUE)
loo1
loo2
```
```{r}
loo_compare(loo1, loo2)
```


Model 2 is better because it has a higher elpd_loo.

## Question 3e

```{r}
loo1_i <- loo1$pointwise[,"elpd_loo"]
loo2_i <- loo2$pointwise[,"elpd_loo"]
loo_i_df <- data.frame(loo1_i, loo2_i)
loo_i_df %>% ggplot(aes(x=loo1_i, y=loo2_i)) + geom_point()
```
```{r}
loo_i_df <- data.frame(loo_i_diff = loo2_i - loo1_i, cent_arsenic = wells$cent_arsenic)
loo_i_df %>% ggplot(aes(x=loo_i_diff, y=cent_arsenic)) + geom_point()
```
  
# Question 3f

