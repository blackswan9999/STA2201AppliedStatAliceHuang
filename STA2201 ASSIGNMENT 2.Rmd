---
title: "STA2201 ASSIGNMENT 2 Alice Huang"
author: "Alice Huang"
date: "05/03/2023"
output: pdf_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
# for bayes stuff
library(here)
library(rstan)
library(bayesplot) 
library(loo) 
library(tidybayes) 
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, echo=FALSE)
```

# Question 1

## Question 1a

We know that if $\mu \sim N(\mu_0 , \sigma^2_{\mu_0})$ and $y_{i}|\mu, \sigma^2 \sim N(\mu, \sigma^2 )$ and $\sigma^2$ is known, then

$$\mu|y,\sigma^2 \sim N(\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}, \frac{1}{1/\sigma^2_{\mu_0} + n/\sigma^2})$$

We are given that $$\mu_0 = 100, \sigma_{\mu_0} = 15, n=10, \sigma = 15, \bar{y} = 113$$

```{r}
mu0 <- 100
sigma_mu0 <- 15
n <- 10
sigma <- 15
ybar <- 113

post_mean <- ((mu0/(sigma_mu0^2)) + (n*ybar/(sigma^2)))/((1/(sigma_mu0^2)) + (n/(sigma^2))) 

post_var <- 1/((1/(sigma_mu0^2))+(n/(sigma^2)))
```


So 

$$\mu|y,\sigma^2 \sim N(\frac{100/15^2 + 10(113)/15^2}{1/15^2 + 10/15^2}, \frac{1}{1/15^2 + 10/15^2}) = N(\frac{1230}{11},\frac{15^2}{11}) = N(111.8182, 20.45455)$$

The Bayesian point estimate is given by the posterior mean $\hat{\mu} = E(\mu|y) = \frac{1230}{11} = 111.8182$.

The 95% credible interval is given by $(\hat{\mu} - q_{0.975}\sigma, \hat{\mu} + q_{0.975}\sigma)$

```{r}
c(post_mean - qnorm(0.975)*sqrt(post_var), post_mean + qnorm(0.975)*sqrt(post_var))
```

$(111.8182 - 1.96\sqrt{20.45455} , 111.8182 + 1.96\sqrt{20.45455}) = (102.9539,120.6825)$

So the 95% credible interval is $(102.9539,120.6825)$.

## Question 1b

$E((\hat{\mu}-\mu^*)^2 | \mu^* ) = E((\hat{\mu}-E(\hat{\mu}|\mu^*)+E(\hat{\mu}|\mu^*)-\mu^*)^2 | \mu^* )$

$= E((\hat{\mu}-E(\hat{\mu}|\mu^*))^2|\mu^*)+ 2E((\hat{\mu}-E(\hat{\mu}|\mu^*))(E(\hat{\mu}|\mu^*)-\mu^*)|\mu^*) + E((E(\hat{\mu}|\mu^*)-\mu^*)^2 | \mu^* )$

$= Var(\hat{\mu}|\mu^* ) + 2 E[\hat{\mu} E(\hat{\mu} | \mu^*) - \hat{\mu}\mu^* - E(\hat{\mu}|\mu^*)^2 + \mu^* E(\hat{\mu}|\mu^*)|\mu^*] + E[E(\hat{\mu}|\mu^*)^2 - 2\mu^* E(\hat{\mu}|\mu^*) + \mu^*|\mu^*]$

$= Var(\hat{\mu}|\mu^* ) + E[2\hat{\mu} E(\hat{\mu} | \mu^*) - 2\hat{\mu}\mu^* - 2E(\hat{\mu}|\mu^*)^2 + 2\mu^* E(\hat{\mu}|\mu^*)+ E(\hat{\mu}|\mu^*)^2 - 2\mu^* E(\hat{\mu}|\mu^*) + \mu^*|\mu^*]$

$= Var(\hat{\mu}|\mu^* ) + 2E(\hat{\mu}|\mu^*) E(\hat{\mu} | \mu^*) - 2E(\hat{\mu}|\mu^*)\mu^* - E(\hat{\mu}|\mu^*)^2 + \mu^*$

$= Var(\hat{\mu}|\mu^* ) + E(\hat{\mu}|\mu^*)^2 - 2E(\hat{\mu}|\mu^*)\mu^* + \mu^*$

$= Var(\hat{\mu}|\mu^* ) + (Bias(\hat{\mu}|\mu^*))^2$

## Question 1c

We derive the distribution of $\hat{\mu}_{BAYES} = \frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}$

Since $y_i | \mu, \sigma^2 \sim N(112, 15^2 )$ we have that 

$E[\hat{\mu}_{BAYES}] = E\bigg[\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}\bigg] = \frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}$

$Var[\hat{\mu}_{BAYES}] = Var\bigg[\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}\bigg] = \frac{(n/\sigma^2 )^2 (\sigma^2 /n)}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2} = \frac{n/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2}$

So $\hat{\mu}_{Bayes} \sim N(\frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}, \frac{n/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2})$

We substitute $$\mu_0 = 100, \sigma_{\mu_0} = 15, n=10, \sigma = 15, \mu = 112$$

```{r}
mu0 <- 100
sigma_mu0 <- 15
n <- 10
sigma <- 15
mu <- 112

mu_hat_bayes_mean <- function(mu0, sigma_mu0, n, sigma, mu){
  ((mu0/(sigma_mu0^2)) + (n*mu/(sigma^2)))/((1/(sigma_mu0^2)) + (n/(sigma^2))) }
mu_hat_bayes_mean(mu0, sigma_mu0, n, sigma, mu)

mu_hat_bayes_var <- function(mu0, sigma_mu0, n, sigma, mu){
  (n/(sigma^2))/(((1/(sigma_mu0^2))+(n/(sigma^2)))^2) }
mu_hat_bayes_var(mu0, sigma_mu0, n, sigma, mu)
```

Then we get 

$$\hat{\mu}_{Bayes} \sim N(\frac{100/(15^2) + 10(112)/15^2}{1/15^2 + 10/15^2}, \frac{10/15^2}{(1/15^2 + 10/15^2)^2}) \sim N(110.9091,18.59504)$$

So 

$$MSE(\hat{\mu}_{Bayes}|\mu^* = 112) = Var(\hat{\mu}|\mu^* ) + (Bias(\hat{\mu}|\mu^*))^2 = 18.59504 + (110.9091 - 112)^2 = 19.7851$$

The bias for $\hat{\mu}_{BAYES}$ is $110.9091 - 112 = -1.0909$. The variance of $\hat{\mu}_{BAYES}$ is $18.59504$.

We have $\mu^* = 112$. The ML estimator is the sample mean, which is $\bar{y}$. $y \sim N(112, 15^2) \implies \bar{y} \sim N(112, \frac{15^2}{10})$.

The bias of the ML estimator is $E(\bar{y}|\mu^*) - 112 = E(y) - 112 = 112-112 = 0$

The variance of the ML estimator is $\frac{15^2}{10} = 22.5$.

The MSE of the ML estimator is $MSE = Var(\bar{y}) + (Bias(\bar{y}))^2 = \frac{Var(y)}{10} + 0= \frac{15^2}{10} = 22.5$

$\hat{\mu}_{BAYES}$ has larger magnitude of bias, lower variance, and lower MSE.

The ML estimator has lower bias, higher variance, and higher MSE.

## Question 1d

We have $\mu^* = 112$. The ML estimator is the sample mean, which is $\bar{y}$. 

$y \sim N(112, 15^2) \implies \bar{y} \sim N(112, \frac{15^2}{10})$.

So the sampling distribution of the ML estimate $\bar{y}$ is $N(112, \frac{15^2}{10})$.

Now we derive the distribution of $\hat{\mu}_{BAYES} = \frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}$

Since $y_i | \mu, \sigma^2 \sim N(\mu, \sigma^2 )$ we have that 

$E[\hat{\mu}_{BAYES}] = E\bigg[\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}\bigg] = \frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}$

$Var[\hat{\mu}_{BAYES}] = Var\bigg[\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}\bigg] = \frac{(n/\sigma^2 )^2 (\sigma^2 /n)}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2} = \frac{n/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2}$

So $\hat{\mu}_{Bayes} \sim N(\frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}, \frac{n/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2})$

```{r}
df = data.frame(x=seq(95, 130, 0.5), 
                ML_sampling_distribution = dnorm(x=seq(95, 130, 0.5) , mean=112, sd = 15/sqrt(10)),
                Bayes_estimator_sampling_distribution = dnorm(x=seq(95, 130, 0.5), mean=110.9091, sd = sqrt(18.59504)))

ggplot(df, aes(x=x, y=ML_sampling_distribution)) + geom_line() + stat_function(fun=dnorm, args = list(110.9091, sqrt(18.59504)), col="red") + geom_vline(xintercept = 112, col="blue")
```

The curve in red represents the Bayesian estimate. We can see that the mean of the Bayesian estimate sampling distribution is farther from the true mean than the mean of the ML estimate sampling distribution. The variance of the ML estimate sampling distribution seems to be slightly larger than the variance of the Bayesian estimate sampling distribution. The bias of the ML estimate is smaller than the bias of the Bayesian estimate because the peak of the Bayesian estimate sampling distribution is at the true mean. The variance of the Bayesian estimate is smaller but the bias of the Bayesian estimate is larger.

$MSE(\bar{y}|\mu) = Var(\bar{y}|\mu) + [E(\bar{y}|\mu)-\mu]^2 = \frac{\sigma^2}{n}$

$MSE(\hat{\mu}_{Bayes}) = Var(\hat{\mu}_{Bayes}|\mu) + [E(\hat{\mu}_{Bayes}|\mu)-\mu]^2 = \frac{n/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2} + \bigg[\frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2} - \mu\bigg]^2$

```{r}
MSE_Bayes_estimate <- function(mu0, sigma_mu0, n, sigma, mu){
  r = mu_hat_bayes_var(mu0, sigma_mu0, n, sigma, mu) + (mu_hat_bayes_mean(mu0, sigma_mu0, n, sigma, mu) - mu)^2
  return(r)
}

MSE_ML <- function(sigma, n){
  result = (sigma^2)/n
  return(result)
}


MSE_ratio <- function(mu0, sigma_mu0, n, sigma, mu){
  result = MSE_Bayes_estimate(mu0, sigma_mu0, n, sigma, mu)/MSE_ML(sigma, n)
  return(result)
}

n = seq(0, 20)
  
plot(n, MSE_ratio(mu0, sigma_mu0, n, sigma, mu), ylab="ratio between Bayesian MSE and ML MSE", xlab="sample size")
```

In general the ratio between the Bayesian estimate's MSE and the ML estimate's MSE is less than or equal to 1. This is expected because the peak of the Bayesian estimate's sampling distribution was to the left of the peak of the ML estimate's sampling distribution. We can see that as sample size increases, the ratio of the MSE of the Bayes estimate to the MSE of the ML estimate increases towards 1. As our sample size increases, our Bayes estimate MSE gets closer to our ML estimate MSE, because we reduce the variance in the ML estimates. As our sample sample size increases, our Bayes estimate also has lower bias.


# Question 2

```{r include=FALSE}
sweden <- read_csv("sweden.csv")
```

## Question 2a

```{r}
sweden %>% ggplot(aes(x=age, y=deaths, color = year)) + geom_point()
```

Over time, the number of deaths for people ages 50-80 has decreased but the number of deaths for people ages 80-100 has risen. It seems that the age with the most deaths has increased over time. This suggests that people are living longer over time.

```{r}
sweden %>% ggplot(aes(x=age, y=deaths/pop, color = year)) + geom_point()
```

We see that as age increases, the mortality rate increases exponentially.

## Question 2b

Note that $\mu_0 = \alpha \exp(\beta (0)) = \alpha$ so $\alpha$ should be the baseline mortality rate. We will later demean the age covariate before fitting our model so $\alpha$ will be the mortality rate for the age 75 group. In 2019, the median mortality rate across all age groups was around $0.0237$. I expect the mortality rate for 2019 to be close to the mortality rate for 2020. So I set my prior for $\alpha$ to be normally distributed with mean $0.02$.

The rate at which mortality increases for every increase in age should be $\frac{\alpha \exp(\beta(x+1))}{\alpha \exp(\beta x)} = \exp(\beta)$. We expect this to be positive, because as people age, they should have a higher chance of dying. 

```{r}
sweden2019 <- sweden %>% filter(year == 2019)
sweden2019 %>% mutate(mort = deaths/pop) -> sweden2019
median(sweden2019$mort)
median(sweden2019$mort[2:51] - sweden2019$mort[1:50])
```

In 2019, the median difference for the mortality rate between consecutive age groups was around 0.0025. We expect differences between consecutive age groups' death rates to get bigger as age increases. We think that setting a prior mean for $\beta$ as 0.0025 should be reasonable.

Mortality rate should be between 0 and 1 so we use a standard deviation of 0.01 so that alpha estimates within 2 standard deviations of the prior mean will fall in (0,1). I think the difference in mortality rate between consecutive age groups should be positive in general, so I set the prior variance for $\beta$ to be 0.001.

I consider $\alpha \sim N(0.02,0.01)$ and $\beta \sim N(0.0025,0.001)$. Then I will simulate mortality rates given these priors and compare them to the 2018 median mortality rate among all age groups.

```{r}
sweden2018 <- sweden %>% filter(year == 2018)
sweden2018 %>% mutate(mort = deaths/pop) -> sweden2018
```

```{r}
sweden2020 <- sweden %>% filter(year == 2020)
# We demean the age to improve the fit of the model. Hopefully this will speed up convergence.
sweden2020$age = sweden2020$age - mean(sweden2020$age)
# demeaning the population doesn't seem to work because we pass in population into log(alpha) + beta*x + log(pop) and log can't take negative values
# sweden2020$pop = sweden2020$pop - mean(sweden2020$pop)
```

```{r}
set.seed(999)
nsims <- 100
alpha <- abs(rnorm(nsims, 0.0015, 0.01))
beta <- abs(rnorm(nsims, 0.005, 0.01))

total_mortality_rate = median(sweden2018$mort)
sigma <- abs(rnorm(nsims, 0, 1))
dsims <- tibble(z_age = (sweden2020$age-mean(sweden2020$age))/sd(sweden2020$age))

for(i in 1:nsims){
  this_mu <- alpha[i]*exp(beta[i]*dsims$z_age) 
  dsims[paste0(i)] <- this_mu + rnorm(nrow(dsims), 0, sigma)
}

dsl <- dsims %>% 
  pivot_longer(`1`:`51`, names_to = "sim", values_to = "sim_mortality_rate")

dsl %>% 
  ggplot(aes(sim_mortality_rate)) + geom_histogram(aes(y = ..density..), bins = 100, fill = "pink", color = "black") + geom_vline(xintercept = total_mortality_rate, color = "blue", lwd = 0.5, lty = 2) +
  theme_bw(base_size = 16) + 
  annotate("text", x=5, y=0.3, label= "2018 Median Mortality rate \n across all age groups", 
           color = "blue", size = 5) +
 xlim(c(-5, 10))
```

Here we find that most of the simulated mortality rates given $\alpha \sim N(0.02,0.01)$ and $\beta \sim N(0.0025,0.001)$ priors are close to the 2018 median mortality rate across all age groups.

## Question 2c

Before fitting the model, we demeaned the age to improve the fit of the model. Hopefully this will speed up convergence and reduce correlation between alpha and beta. Demeaning the population doesn't seem to work because some of the demeaned population values will be negative. We pass in population into the model $\mu_x = log(\alpha) + \beta*age_x + log(pop_x)$ and log can't take negative values.

```{r}
stan_data_s <- list(N = nrow(sweden2020),
                  deaths = sweden2020$deaths,
                  age = sweden2020$age,
                  pop = sweden2020$pop)

sweden_model <- stan(data = stan_data_s, 
             file = here("sweden.stan"),
             iter = 2000,
             seed = 0)
```

We see that the model converges as the traceplot shows that the chains mixed well and there are no obvious trends.

```{r}
traceplot(sweden_model, pars=c("alpha", "beta"))
```

Demeaning the age covariate helped reduce correlation between alpha and beta. If we had not demeaned the age covariate, there would have been very strong correlation between alpha and beta.

```{r}
pairs(sweden_model, pars = c("alpha", "beta"))
```

We see that the distributions for alpha and beta generated by each chain are similar.

```{r}
stan_dens(sweden_model, separate_chains = TRUE, pars = c("alpha", "beta"))
```

Here is the summary of Model 1.

```{r}
summary(sweden_model)[["summary"]][c("alpha","beta"), ]
```


The mortality rate for age group 75 is $0.0283097$. The ratio of mortality rate between age groups 1 year apart is $\exp(0.1090507)$

## Question 2d

We compare the densities of 100 sampled datasets to the actual data. The densities of the sampled datasets based on the model are somewhat close to the true data.

```{r}
set.seed(0)
y <- sweden2020$deaths
yrep1 <- extract(sweden_model)[["log_deaths_rep"]]
samp100 <- sample(nrow(yrep1), 100)
ppc_dens_overlay(y, yrep1[samp100, ])
```

The median deaths and replicated median deaths are somewhat close to each other.

```{r}
ppc_stat(y, yrep1, stat = 'median')
```

## Question 2e

Again, we demean the age covariate for the entire Sweden mortality dataset, in order to speed up convergence and reduce correlation between $\alpha, \beta$. We create a matrix of death counts where the ij-th element corresponds to the i-th age and j-th year. We extend the previous model to this matrix. We use the same alpha and beta priors.

```{r}
sweden$age <- sweden$age - mean(sweden$age)
N=length(unique(sweden$age))
M=length(unique(sweden$year))

death_matrix = matrix(sweden$deaths,
                      nrow=N, 
                      ncol=M, 
                      byrow = FALSE)

pop_matrix = matrix(sweden$pop, 
                         nrow=N, 
                         ncol=M, 
                         byrow = FALSE)

age_matrix <- matrix(sweden$age, nrow=N, 
                         ncol=M, byrow=FALSE)
```
```{r}
stan_data_matrix <- list(N = N,
                     M = M,
                  deaths = death_matrix,
                  age = age_matrix,
                  pop = pop_matrix)

sweden_model_matrix <- stan(data = stan_data_matrix, 
             file = here("swedenmatrix.stan"),
             iter = 1000,
             seed = 0)
```

The model seems to have converged.

```{r}
traceplot(sweden_model_matrix)
```


```{r}
summary(sweden_model_matrix)[["summary"]][c(paste0("alpha[",1:31, "]")),1] -> alpha_means
summary(sweden_model_matrix)[["summary"]][c(paste0("alpha[",1:31, "]")),4] -> alpha_lower
summary(sweden_model_matrix)[["summary"]][c(paste0("alpha[",1:31, "]")),8] -> alpha_upper
summary(sweden_model_matrix)[["summary"]][c(paste0("beta[",1:31, "]")),1] -> beta_means
summary(sweden_model_matrix)[["summary"]][c(paste0("beta[",1:31, "]")),4] -> beta_lower
summary(sweden_model_matrix)[["summary"]][c(paste0("beta[",1:31, "]")),8] -> beta_upper
year = seq(1990, 2020, by=1)
alpha_df <- data.frame(year,alpha_means, alpha_lower, alpha_upper)
beta_df <- data.frame(year,beta_means, beta_lower, beta_upper)

alpha_df %>% ggplot(aes(x=year, y=alpha_means)) + geom_point() + geom_errorbar(aes(ymin=alpha_lower, ymax=alpha_upper))

beta_df %>% ggplot(aes(x=year, y=beta_means)) + geom_point() + geom_errorbar(aes(ymin=beta_lower, ymax=beta_upper))
```

It seems that over time, the model estimated lower $\alpha$ coefficients. This suggests that the mortality rate for the age 75 group decreased over time. Over time, the model estimated higher $\beta$ coefficients. This suggests that the ratio between mortality rates of age groups 1 year apart increased. 

## Question 2f

```{r}
life_exp = data.frame(life_expectancy = rep(0, 31))

for(i in 40:100){
  # model previously demeaned age so we do i-75
  mu_ai = alpha_means*exp(beta_means*(i-75))
  summand = exp(-mu_ai)
  life_exp = life_exp + summand
}

life_exp_lower = data.frame(life_expectancy_lower = rep(0, 31))

for(i in 40:100){
  # model previously demeaned age so we do i-75
  mu_ai = alpha_lower*exp(beta_lower*(i-75))
  summand = exp(-mu_ai)
  life_exp_lower = life_exp_lower + summand
}

life_exp_upper = data.frame(life_expectancy_upper = rep(0, 31))

for(i in 40:100){
  # model previously demeaned age so we do i-75
  mu_ai = alpha_upper*exp(beta_upper*(i-75))
  summand = exp(-mu_ai)
  life_exp_upper = life_exp_upper + summand
}


year = seq(1990, 2020, by=1)

life_exp = cbind(year, life_exp, life_exp_lower, life_exp_upper)

life_exp %>% ggplot(aes(x=year, y=life_expectancy)) + 
  geom_point() +
  geom_errorbar(aes(ymin = life_expectancy_lower, ymax = life_expectancy_upper)) +
  labs(y="Life expectancy for person aged 40")
```

We can see that over time, the predicted life expectancy for a person aged 40 increases. The life expectancy estimates are rather high however, because the model predicts someone aged 40 living until their late 90s (between 56-58 years of life left at age 40).

# Question 3

```{r, echo=FALSE, warning = FALSE, message=FALSE}
wells <- read.table("http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat", sep=" ")
```

## Question 3a

```{r}
wells %>% ggplot(aes(x=dist, col=factor(switch))) + geom_boxplot()
```

It seems that the median distance of people who switched their well was lower than median distance of people who did not switch their well.

```{r}
wells %>% ggplot(aes(x=arsenic, col=factor(switch))) + geom_boxplot()
```

The median arsenic level was higher for people who switched their wells than for people who did not switch their wells.

```{r}
wells %>% ggplot(aes(x=dist, y=arsenic, col=factor(switch))) + geom_point() + geom_smooth() +theme_bw()
```

Distance and arsenic did not appear to be heavily correlated. However it seems that when arsenic levels were higher, more people switched their wells. When distance was greater, more people did not switch their well.

## Question 3b

```{r}
wells %>% mutate(cent_dist = dist - mean(dist), cent_arsenic = arsenic - mean(arsenic)) %>%
  mutate(cent_dist_cent_arsenic = cent_dist*cent_arsenic) -> wells
```

```{r}
stan_data <- list(N = nrow(wells),
                  switch_well = wells$switch,
                  cent_dist = wells$cent_dist,
                  cent_arsenic = wells$cent_arsenic,
                  cent_dist_cent_arsenic = wells$cent_dist_cent_arsenic)

wells1 <- stan(data = stan_data, 
             file = here("wells.stan"),
             iter = 1000,
             seed = 0)

```

```{r}
wells %>% mutate(cent_log_arsenic = log(arsenic) - mean(log(arsenic))) %>%
  mutate(cent_dist_cent_log_arsenic = cent_dist*cent_log_arsenic) -> wells
```

```{r}
stan_data2 <- list(N = nrow(wells),
                  switch_well = wells$switch,
                  cent_dist = wells$cent_dist,
                  cent_log_arsenic = wells$cent_log_arsenic,
                  cent_dist_cent_log_arsenic = wells$cent_dist_cent_log_arsenic)

wells2 <- stan(data = stan_data2, 
             file = here("wells2.stan"),
             iter = 1000,
             seed = 0)
```

```{r}
traceplot(wells1, pars = c("beta[1]", "beta[2]", "beta[3]", "beta[4]"))
traceplot(wells2, pars = c("beta[1]", "beta[2]", "beta[3]", "beta[4]"))
```

```{r}
summary(wells1)$summary[c("beta[1]", "beta[2]", "beta[3]", "beta[4]"),]
```

```{r}
#summary(wells2)$summary[c("beta[1]", "beta[2]", "beta[3]", "beta[4]"),]
```



The odds of switching a well for someone with mean arsenic levels and mean distance is  $\exp(0.350705008) \approx 1.42$. So someone with mean arsenic levels and mean distance is 42% more likely to switch their well.

For every unit increase in distance, the odds of switching a well are multiplied by $\exp(-0.008739881) \approx 0.99$, assuming all other factors are held constant. Since this is close to 1, someone with a unit increase in distance is probably unlikely to switch their well, assuming other factors held constant.

For every unit increase in arsenic levels, the odds of switching a well are multiplied by $\exp(0.469091474) \approx 1.60$, assuming all other factors are held constant. If there is a unit increase in arsenic, someone is 59.8% more likely to switch their well, assuming other factors held constant.

For every unit increase in demeaned distance multiplied by demeaned arsenic levels, the odds of switching a well are multiplied by $exp(-0.001775180) \approx 0.998$, assuming all other factors are held constant. As seen in the previous graph of arsenic against distance, there isn't much correlation. So arsenic and distance together don't have much influence on switching wells.

## Question 3c

```{r}
t_test_stat = function(y,a){
  num = sum(y==1 & a< 0.82)
  denom = sum(a < 0.82)
  return(num/denom)
}
```

```{r}
yrep1 <- extract(wells1)[["log_switch_rep"]]
yrep2 <- extract(wells2)[["log_switch_rep"]]
```

```{r}
teststat = t_test_stat(wells$switch, wells$arsenic)
teststat_rep <- sapply(1:nrow(yrep1), function(i)t_test_stat(yrep1[i,],wells$arsenic))
teststat_rep_2 <- sapply(1:nrow(yrep2), function(i) t_test_stat(yrep2[i,],wells$arsenic))

ggplot(data = as_tibble(teststat_rep), aes(value)) + 
    geom_histogram(aes(fill = "replicated"), bins=50) + 
    geom_vline(aes(xintercept = teststat, color = "observed"), lwd = 1.5) + 
  ggtitle("Model 1: test stat") + 
  scale_color_manual(name = "", values = c("observed" = "purple"))+
  scale_fill_manual(name = "", values = c("replicated" = "pink")) + theme_bw()

ggplot(data = as_tibble(teststat_rep_2), aes(value)) + 
    geom_histogram(aes(fill = "replicated"), bins = 50) + 
    geom_vline(aes(xintercept = teststat, color = "observed"), lwd = 1.5) + 
  ggtitle("Model 2: test stat") + 
  scale_color_manual(name = "", values = c("observed" = "purple"))+
  scale_fill_manual(name = "", values = c("replicated" = "pink")) + theme_bw()

```

Model 2 was better, because most of the replicated test statistics are closer to the observed test statistic.

## Question 3d

```{r}
library(loo)
loglik1 <- extract(wells1)[["log_lik"]]
loglik2 <- extract(wells2)[["log_lik"]]
loo1 <- loo(loglik1, save_psis = TRUE)
loo2 <- loo(loglik2, save_psis = TRUE)
loo1
loo2
```
```{r}
loo_compare(loo1, loo2)
```


Model 2 is better because it has a higher elpd_loo.

## Question 3e

```{r}
loo1_i <- loo1$pointwise[,1]
loo2_i <- loo2$pointwise[,1]
loo_i_df <- data.frame(loo1_i, loo2_i, y = as.factor(wells$switch))
loo_i_df %>% ggplot(aes(x=loo1_i, y=loo2_i, color=y)) + geom_point() 
```
```{r}
loo_i_diff <- loo2_i - loo1_i
loo_i_df <- data.frame(loo_i_diff = loo_i_diff, 
                       log_arsenic = log(wells$arsenic), 
                       y=factor(wells$switch))
loo_i_df %>% ggplot(aes(x=loo_i_diff, y=log_arsenic, color = y)) + geom_point()
```
  
If the difference between $ELPD_i$ for model 2 and model 1 is positive, Model 2 is better. Model 2 is better for households who did not switch wells and had extremely high arsenic or extremely low arsenic.  Model 2 is better for households who switched wells and had arsenic level between 0 and 1. 
  
## Question 3f

$exp(ELPD_i )$ is the probability of switching wells for house i.

## Question 3g

```{r}
yi_hat_1 <- exp(loo1_i)
yi_hat_2 <- exp(loo2_i)
resid_1 <- wells$switch - yi_hat_1
resid_2 <- wells$switch - yi_hat_2
log_arsenic <- log(wells$arsenic)
log_arsenic_binned <- cut(log_arsenic, breaks = 40)
arsenic_binned <- cut(wells$arsenic, breaks = 40)
```

```{r}
dataframe1 <- data.frame(resid_1, arsenic_binned)
dataframe1 %>% group_by(arsenic_binned) %>%
  summarize(mean_resid_ars_group =  mean(resid_1), se_resid_ars_group = sd(resid_1)/sqrt(length(resid_1))) -> newdataframe1

newdataframe1 %>% ggplot(aes(x=arsenic_binned, y=mean_resid_ars_group)) + geom_point() + geom_errorbar(aes(ymin = mean_resid_ars_group - 2*se_resid_ars_group, ymax = mean_resid_ars_group + 2*se_resid_ars_group)) + labs(x="Arsenic level", y="Mean residual by arsenic group")+ theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```

If the mean residual for a certain arsenic level range is close to 0, the model is predicting well. 

```{r}
dataframe2 <- data.frame(resid_2, log_arsenic_binned)
dataframe2 %>% group_by(log_arsenic_binned) %>%
  summarize(mean_resid_logars_group =  mean(resid_2), se_resid_logars_group = sd(resid_2)/sqrt(length(resid_2))) -> newdataframe1

newdataframe1 %>% ggplot(aes(x=log_arsenic_binned, y=mean_resid_logars_group)) + geom_point() + geom_errorbar(aes(ymin = mean_resid_logars_group - 2*se_resid_logars_group, ymax = mean_resid_logars_group + 2*se_resid_logars_group)) + labs(x="Log Arsenic level", y="Mean residual by log arsenic group") + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```