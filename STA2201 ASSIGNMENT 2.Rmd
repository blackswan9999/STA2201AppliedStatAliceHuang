---
title: "STA2201 ASSIGNMENT 2 Alice Huang"
author: "Alice Huang"
date: "05/03/2023"
output: pdf_document
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
# for bayes stuff
library(here)
library(rstan)
library(bayesplot) 
library(loo) 
library(tidybayes) 
```


# Question 1

## Question 1a

We know that if $\mu \sim N(\mu_0 , \sigma^2_{\mu_0})$ and $y_{i}|\mu, \sigma^2 \sim N(\mu, \sigma^2 )$ and $\sigma^2$ is known, then

$$\mu|y,\sigma^2 \sim N(\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}, \frac{1}{1/\sigma^2_{\mu_0} + n/\sigma^2})$$

We are given that $$\mu_0 = 100, \sigma_{\mu_0} = 15, n=10, \sigma = 15, \bar{y} = 113$$

```{r}
mu0 <- 100
sigma_mu0 <- 15
n <- 10
sigma <- 15
ybar <- 113

post_mean <- ((mu0/(sigma_mu0^2)) + (n*ybar/(sigma^2)))/((1/(sigma_mu0^2)) + (n/(sigma^2))) 

post_var <- 1/((1/(sigma_mu0^2))+(n/(sigma^2)))
```


So 

$$\mu|y,\sigma^2 \sim N(\frac{100/15^2 + 10(113)/15^2}{1/15^2 + 10/15^2}, \frac{1}{1/15^2 + 10/15^2}) = N(\frac{1230}{11},\frac{15^2}{11}) = N(111.8182, 20.45455)$$

The Bayesian point estimate is given by the posterior mean $\hat{\mu} = E(\mu|y) = \frac{1230}{11} = 111.8182$.

The 95% credible interval is given by $(\hat{\mu} - q_{0.975}\sigma, \hat{\mu} + q_{0.975}\sigma)$

```{r}
c(post_mean - qnorm(0.975)*sqrt(post_var), post_mean + qnorm(0.975)*sqrt(post_var))
```

$(111.8182 - 1.96\sqrt{20.45455} , 111.8182 + 1.96\sqrt{20.45455}) = (102.9539,120.6825)$

So the 95% credible interval is $(102.9539,120.6825)$.

## Question 1b

$E((\hat{\mu}-\mu^*)^2 | \mu^* ) = E((\hat{\mu}-E(\hat{\mu}|\mu^*)+E(\hat{\mu}|\mu^*)-\mu^*)^2 | \mu^* )$

$= E((\hat{\mu}-E(\hat{\mu}|\mu^*))^2|\mu^*)+ 2E((\hat{\mu}-E(\hat{\mu}|\mu^*))(E(\hat{\mu}|\mu^*)-\mu^*)|\mu^*) + E((E(\hat{\mu}|\mu^*)-\mu^*)^2 | \mu^* )$

$= Var(\hat{\mu}|\mu^* ) + 2 E[\hat{\mu} E(\hat{\mu} | \mu^*) - \hat{\mu}\mu^* - E(\hat{\mu}|\mu^*)^2 + \mu^* E(\hat{\mu}|\mu^*)|\mu^*] + E[E(\hat{\mu}|\mu^*)^2 - 2\mu^* E(\hat{\mu}|\mu^*) + \mu^*|\mu^*]$

$= Var(\hat{\mu}|\mu^* ) + E[2\hat{\mu} E(\hat{\mu} | \mu^*) - 2\hat{\mu}\mu^* - 2E(\hat{\mu}|\mu^*)^2 + 2\mu^* E(\hat{\mu}|\mu^*)+ E(\hat{\mu}|\mu^*)^2 - 2\mu^* E(\hat{\mu}|\mu^*) + \mu^*|\mu^*]$

$= Var(\hat{\mu}|\mu^* ) + 2E(\hat{\mu}|\mu^*) E(\hat{\mu} | \mu^*) - 2E(\hat{\mu}|\mu^*)\mu^* - E(\hat{\mu}|\mu^*)^2 + \mu^*$

$= Var(\hat{\mu}|\mu^* ) + E(\hat{\mu}|\mu^*)^2 - 2E(\hat{\mu}|\mu^*)\mu^* + \mu^*$

$= Var(\hat{\mu}|\mu^* ) + (Bias(\hat{\mu}|\mu^*))^2$

## Question 1c

We derive the distribution of $\hat{\mu}_{BAYES} = \frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}$

Since $y_i | \mu, \sigma^2 \sim N(112, 15^2 )$ we have that 

$E[\hat{\mu}_{BAYES}] = E\bigg[\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}\bigg] = \frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}$

$Var[\hat{\mu}_{BAYES}] = Var\bigg[\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}\bigg] = \frac{(n/\sigma^2 )^2 (\sigma^2 /n)}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2} = \frac{n/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2}$

So $\hat{\mu}_{Bayes} \sim N(\frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}, \frac{n/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2})$

Then we get 

$$\hat{\mu}_{Bayes} \sim N(\frac{100/(15^2) + 10(112)/15^2}{1/15^2 + 10/15^2}, \frac{10/15^2}{(1/15^2 + 10/15^2)^2}) \sim N(110.9091,18.59504)$$

So 

$$MSE(\hat{\mu}_{Bayes}|\mu^* = 112) = Var(\hat{\mu}|\mu^* ) + (Bias(\hat{\mu}|\mu^*))^2 = 18.59504 + (110.9091 - 112)^2 = 19.7851$$

The bias for $\hat{\mu}_{BAYES}$ is -1.0909.

We have $\mu^* = 112$. The ML estimator is the sample mean, which is $\bar{y}$. $y \sim N(112, 15^2) \implies \bar{y} \sim N(112, \frac{15^2}{10})$.

The bias of the ML estimator is $E(\bar{y}) - 112 = E(y) - 112 = 0$

The MSE of the ML estimator is $MSE = Var(\bar{y}) + (Bias(\bar{y}))^2 = \frac{Var(y)}{10} + 0= \frac{15^2}{10} = 22.5$

$\hat{\mu}_{BAYES}$ has larger bias, lower variance, and lower MSE.

The ML estimator has lower bias, higher variance, and higher MSE.

## Question 1d

We have $\mu^* = 112$. The ML estimator is the sample mean, which is $\bar{y}$. $y \sim N(112, 15^2) \implies \bar{y} \sim N(112, \frac{15^2}{10})$.

Now we derive the distribution of $\hat{\mu}_{BAYES} = \frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}$

Since $y_i | \mu, \sigma^2 \sim N(112, 15^2 )$ we have that 

$E[\hat{\mu}_{BAYES}] = E\bigg[\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}\bigg] = \frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}$

$Var[\hat{\mu}_{BAYES}] = Var\bigg[\frac{\mu_{0}/\sigma^2_{\mu_0} + n\bar{y}/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}\bigg] = \frac{(n\mu/\sigma^2 )^2 (\sigma^2 /n)}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2} = \frac{n\mu/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2}$

So $\hat{\mu}_{Bayes} \sim N(\frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}, \frac{n\mu/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2})$

Then the sample mean of $\hat{\mu}_{Bayes}$, $\bar{\hat{\mu}}_{Bayes}$ follows the following distribution:
$$N(\frac{\mu_{0}/\sigma^2_{\mu_0} + n\mu/\sigma^2}{1/\sigma^2_{\mu_0} + n/\sigma^2}, \frac{1/\sigma^2}{(1/\sigma^2_{\mu_0} + n/\sigma^2)^2})$$

We substitute $$\mu_0 = 100, \sigma_{\mu_0} = 15, n=10, \sigma = 15, \mu = 112$$

```{r}
mu0 <- 100
sigma_mu0 <- 15
n <- 10
sigma <- 15
mu <- 112

mu_hat_bayes_bar_mean <- function(mu0, sigma_mu0, n, sigma, mu){
  ((mu0/(sigma_mu0^2)) + (n*mu/(sigma^2)))/((1/(sigma_mu0^2)) + (n/(sigma^2))) }
mu_hat_bayes_bar_mean(mu0, sigma_mu0, n, sigma, mu)

mu_hat_bayes_bar_var <- function(mu0, sigma_mu0, n, sigma, mu){
  (1/(sigma^2))/(((1/(sigma_mu0^2))+(n/(sigma^2)))^2) }
mu_hat_bayes_bar_var(mu0, sigma_mu0, n, sigma, mu)
```

Then we get 

$$\bar{\hat{\mu}}_{Bayes} \sim N(\frac{100/(15^2) + 10(112)/15^2}{1/15^2 + 10/15^2}, \frac{1/15^2}{(1/15^2 + 10/15^2)^2}) \sim N(110.9091,1.859504)$$

```{r}
df = data.frame(x=seq(95, 130, 0.5), 
                ML_sampling_distribution = dnorm(x=seq(95, 130, 0.5) , mean=112, sd = 15/sqrt(10)),
                Bayes_estimator_sampling_distribution = dnorm(x=seq(95, 130, 0.5), mean=110.9091, sd = sqrt(18.59504)))

ggplot(df, aes(x=x, y=ML_sampling_distribution)) + geom_line() + stat_function(fun=dnorm, args = list(110.9091, sqrt(18.59504)), col="red")
```

The curve in red represents the Bayesian estimate. We can see that the mean of the Bayesian estimate sampling distribution is lower than the mean of the ML estimate sampling distribution. The variance of the ML estimate sampling distribution seems to be slightly larger and the bias of the ML estimate is smaller. The variance of the Bayesian estimate is smaller but the bias of the Bayesian estimate is larger.

$MSE(\bar{y}|\mu) = Var(\bar{y}|\mu) + [E(\bar{y}|\mu)-\mu]^2 = \frac{\sigma^2}{n}$

$MSE(\hat{\mu}_{Bayes}) = Var(\bar{y}|\mu) + [E(\hat{\mu}_{Bayes}|\mu)-\mu]^2$

```{r}
MSE_Bayes_estimate <- function(mu0, sigma_mu0, n, sigma, mu){
  r = n*mu_hat_bayes_bar_var(mu0, sigma_mu0, n, sigma, mu) + (mu_hat_bayes_bar_mean(mu0, sigma_mu0, n, sigma, mu) - mu)^2
  return(r)
}

MSE_ML <- function(sigma, n){
  result = (sigma^2)/n
  return(result)
}


MSE_ratio <- function(mu0, sigma_mu0, n, sigma, mu){
  result = MSE_Bayes_estimate(mu0, sigma_mu0, n, sigma, mu)/MSE_ML(sigma, n)
  return(result)
}

n = seq(0, 20)
  
plot(n, MSE_ratio(mu0, sigma_mu0, n, sigma, mu), ylab="MSE ratio")
```

We can see that as sample size increases, the ratio of the MSE of the Bayes estimate to the MSE of the ML estimate increases. As we have more data our ML estimate gets closer to our Bayes estimate.


# Question 2

```{r include=FALSE}
sweden <- read_csv("sweden.csv")
```
## Question 2a

```{r}
sweden %>% ggplot(aes(x=age, y=deaths, color = year)) + geom_point()
```

Over time, the number of deaths for people ages 50-80 has decreased but the number of deaths for people ages 80-100 has risen. It seems that the age with the most deaths has increased over time. This suggests that people are living longer over time.

```{r}
sweden %>% ggplot(aes(x=age, y=deaths/pop, color = year)) + geom_point()
```

We see that as age increases, the mortality rate increases exponentially.

## Question 2b

```{r}
sweden2020 <- sweden %>% filter(year == 2020)
sweden2020$age <- sweden2020$age-50
```

```{r}
set.seed(999)
nsims <- 100
alpha <- abs(rnorm(nsims, 0.0015, 0.01))
beta <- abs(rnorm(nsims, 0.005, 0.01))
sweden2019 <- sweden %>% filter(year == 2019)
total_mortality_rate = median((sweden2019$deaths)/(sweden2019$pop))
sigma <- abs(rnorm(nsims, 0, 1))
dsims <- tibble(z_age = (sweden2020$age-mean(sweden2020$age))/sd(sweden2020$age))

for(i in 1:nsims){
  this_mu <- alpha[i]*exp(beta[i]*dsims$z_age) 
  dsims[paste0(i)] <- this_mu + rnorm(nrow(dsims), 0, sigma)
}

dsl <- dsims %>% 
  pivot_longer(`1`:`51`, names_to = "sim", values_to = "sim_mortality_rate")

#max_mortality_rate = max(sweden2020$deaths/sweden2020$pop)

dsl %>% 
  ggplot(aes(sim_mortality_rate)) + geom_histogram(aes(y = ..density..), bins = 100, fill = "pink", color = "black") + geom_vline(xintercept = total_mortality_rate, color = "blue", lwd = 0.5, lty = 2) +
  theme_bw(base_size = 16) + 
  annotate("text", x=10, y=0.2, label= "2019 Median Mortality rate \n across all age groups", 
           color = "purple", size = 5) +
 xlim(c(-10, 25))

#+ geom_vline(xintercept = max_mortality_rate, color = "red", lwd = 0.5, lty = 2)+
```

$\alpha$ should be the baseline mortality rate, ie the mortality rate for the age 50 group. In 2019, the mortality rate for the age 50 group was $200/132747 \approx 0.0015$. I expect the mortality rate for 2019 to be close to the mortality rate for 2020. So I set my prior for $\alpha$ to be normally distributed with mean $0.0015$.

$\exp(\beta) = \frac{\alpha \exp(\beta(x+1))}{\alpha \exp(\beta x)}$ should be the rate at which mortality increases for every increase in age. We expect this to be positive, because as people age, they should have a higher chance of dying. In 2019, the difference for the death rate for the age 51 group and the age 50 group is around 0.0002. We expect differences between consecutive age groups' death rates to get bigger as age increases. We think that setting a prior mean for $\beta$ as 0.005 should be reasonable.

Mortality rate should not exceed 1 so we use a standard deviation of 0.01.

I consider $\alpha \sim N(0.0025,0.01)$ and $\beta \sim N(0.005,0.01)$

## Question 2c

```{r}
stan_data_s <- list(N = nrow(sweden2020),
                  deaths = sweden2020$deaths,
                  age = sweden2020$age,
                  pop = sweden2020$pop)

sweden_model <- stan(data = stan_data_s, 
             file = here("sweden.stan"),
             iter = 3000,
             seed = 0)
```

```{r}
traceplot(sweden_model, pars=c("alpha", "beta"))
```
```{r}
stan_dens(sweden_model, separate_chains = TRUE)
```
```{r}
summary(sweden_model)[["summary"]][c("alpha","beta"), ]
```


The mortality rate for age 50 is $0.001343977$. The ratio of mortality rate for every increase in age is $\exp(0.119449114)$

## Question 2d

```{r}
set.seed(1)
y <- sweden$deaths
yrep1 <- extract(sweden_model)[["log_deaths_rep"]]
samp100 <- sample(nrow(yrep1), 100)
ppc_dens_overlay(y, yrep1[samp100, ])
```

The observed deaths and replicated deaths are close to each other.

```{r}
ppc_stat(y, yrep1, stat = 'median')
```

## Question 2e

```{r}
sweden$age <- sweden$age - mean(sweden$age)
N=length(unique(sweden$age))
M=length(unique(sweden$year))

```
```{r}
death_matrix = matrix(sweden$deaths, 
                         nrow=N, 
                         ncol=M, 
                         byrow = FALSE)

pop_matrix = matrix(sweden$pop, 
                         nrow=N, 
                         ncol=M, 
                         byrow = FALSE)

age_matrix <- matrix(sweden$age, nrow=N, 
                         ncol=M, byrow=FALSE)
```
```{r}
stan_data_matrix <- list(N = N,
                     M = M,
                  deaths = death_matrix,
                  age = age_matrix,
                  pop = pop_matrix)

sweden_model_matrix <- stan(data = stan_data_matrix, 
             file = here("swedenmatrix.stan"),
             iter = 1000,
             seed = 0)
```

```{r}
summary(sweden_model_matrix)[["summary"]][c(paste0("alpha[",1:31, "]")),1] -> alpha_means
summary(sweden_model_matrix)[["summary"]][c(paste0("alpha[",1:31, "]")),4] -> alpha_lower
summary(sweden_model_matrix)[["summary"]][c(paste0("alpha[",1:31, "]")),8] -> alpha_upper
summary(sweden_model_matrix)[["summary"]][c(paste0("beta[",1:31, "]")),1] -> beta_means
summary(sweden_model_matrix)[["summary"]][c(paste0("beta[",1:31, "]")),4] -> beta_lower
summary(sweden_model_matrix)[["summary"]][c(paste0("beta[",1:31, "]")),8] -> beta_upper
year = seq(1990, 2020, by=1)
alpha_df <- data.frame(year,alpha_means, alpha_lower, alpha_upper)
beta_df <- data.frame(year,beta_means, beta_lower, beta_upper)
```
```{r}
alpha_df %>% ggplot(aes(x=year, y=alpha_means)) + geom_point() + geom_errorbar(aes(ymin=alpha_lower, ymax=alpha_upper))
```
```{r}
beta_df %>% ggplot(aes(x=year, y=beta_means)) + geom_point() + geom_errorbar(aes(ymin=beta_lower, ymax=beta_upper))
```

It seems that over time, the model estimated lower $\alpha$ coefficients. Over time, the model estimated higher $\beta$ coefficients.

## Question 2f

```{r}
age = 40:100
lexp = c()
for(age_i in age){
  mort <- alpha_means*exp(beta_means*age_i)
  lexp[age_i-40+1] = exp(-mort) + lexp[age_i-40]
}
plot(year, lexp)
```


# Question 3

```{r, echo=FALSE, warning = FALSE, message=FALSE}
wells <- read.table("http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat", sep=" ")
```

## Question 3a

```{r}
wells %>% ggplot(aes(x=dist, col=factor(switch))) + geom_boxplot()
```

It seems that the median distance of people who switched their well was lower than median distance of people who did not switch their well.

```{r}
wells %>% ggplot(aes(x=arsenic, col=factor(switch))) + geom_boxplot()
```

The median arsenic level was higher for people who switched their wells than for people who did not switch their wells.

```{r}
wells %>% ggplot(aes(x=dist, y=arsenic, col=factor(switch))) + geom_point() + geom_smooth() +theme_bw()
```

Distance and arsenic did not appear to be heavily correlated. However it seems that when arsenic levels were higher, more people switched their wells. When distance was greater, more people did not switch their well.

## Question 3b

```{r}
wells %>% mutate(cent_dist = dist - mean(dist), cent_arsenic = arsenic - mean(arsenic)) %>%
  mutate(cent_dist_cent_arsenic = cent_dist*cent_arsenic) -> wells
```

```{r}
stan_data <- list(N = nrow(wells),
                  switch_well = wells$switch,
                  cent_dist = wells$cent_dist,
                  cent_arsenic = wells$cent_arsenic,
                  cent_dist_cent_arsenic = wells$cent_dist_cent_arsenic)

wells1 <- stan(data = stan_data, 
             file = here("wells.stan"),
             iter = 1000,
             seed = 0)

```

```{r}
wells %>% mutate(cent_log_arsenic = log(arsenic) - mean(log(arsenic))) %>%
  mutate(cent_dist_cent_log_arsenic = cent_dist*cent_log_arsenic) -> wells
```

```{r}
stan_data2 <- list(N = nrow(wells),
                  switch_well = wells$switch,
                  cent_dist = wells$cent_dist,
                  cent_log_arsenic = wells$cent_log_arsenic,
                  cent_dist_cent_log_arsenic = wells$cent_dist_cent_log_arsenic)

wells2 <- stan(data = stan_data2, 
             file = here("wells2.stan"),
             iter = 1000,
             seed = 0)
```

```{r}
traceplot(wells1, pars = c("beta[1]", "beta[2]", "beta[3]", "beta[4]"))
traceplot(wells2, pars = c("beta[1]", "beta[2]", "beta[3]", "beta[4]"))
```

```{r}
summary(wells1)$summary[c("beta[1]", "beta[2]", "beta[3]", "beta[4]"),]
```

```{r}
#summary(wells2)$summary[c("beta[1]", "beta[2]", "beta[3]", "beta[4]"),]
```



The odds of switching a well for someone with mean arsenic levels and mean distance is  $\exp(0.350705008) \approx 1.42$. So someone with mean arsenic levels and mean distance is 42% more likely to switch their well.

For every unit increase in distance, the odds of switching a well are multiplied by $\exp(-0.008739881) \approx 0.99$, assuming all other factors are held constant. Since this is close to 1, someone with a unit increase in distance is probably unlikely to switch their well, assuming other factors held constant.

For every unit increase in arsenic levels, the odds of switching a well are multiplied by $\exp(0.469091474) \approx 1.60$, assuming all other factors are held constant. If there is a unit increase in arsenic, someone is 59.8% more likely to switch their well, assuming other factors held constant.

For every unit increase in demeaned distance multiplied by demeaned arsenic levels, the odds of switching a well are multiplied by $exp(-0.001775180) \approx 0.998$, assuming all other factors are held constant. As seen in the previous graph of arsenic against distance, there isn't much correlation. So arsenic and distance together don't have much influence on switching wells.

## Question 3c

```{r}
t_test_stat = function(y,a){
  num = sum(y==1 & a< 0.82)
  denom = sum(a < 0.82)
  return(num/denom)
}
```

```{r}
yrep1 <- extract(wells1)[["log_switch_rep"]]
yrep2 <- extract(wells2)[["log_switch_rep"]]
```

```{r}
teststat = t_test_stat(wells$switch, wells$arsenic)
teststat_rep <- sapply(1:nrow(yrep1), function(i)t_test_stat(yrep1[i,],wells$arsenic))
teststat_rep_2 <- sapply(1:nrow(yrep2), function(i) t_test_stat(yrep2[i,],wells$arsenic))

ggplot(data = as_tibble(teststat_rep), aes(value)) + 
    geom_histogram(aes(fill = "replicated"), bins=50) + 
    geom_vline(aes(xintercept = teststat, color = "observed"), lwd = 1.5) + 
  ggtitle("Model 1: test stat") + 
  scale_color_manual(name = "", values = c("observed" = "purple"))+
  scale_fill_manual(name = "", values = c("replicated" = "pink")) + theme_bw()

ggplot(data = as_tibble(teststat_rep_2), aes(value)) + 
    geom_histogram(aes(fill = "replicated"), bins = 50) + 
    geom_vline(aes(xintercept = teststat, color = "observed"), lwd = 1.5) + 
  ggtitle("Model 2: test stat") + 
  scale_color_manual(name = "", values = c("observed" = "purple"))+
  scale_fill_manual(name = "", values = c("replicated" = "pink")) + theme_bw()

```

Model 2 was better, because most of the replicated test statistics are closer to the observed test statistic.

## Question 3d

```{r}
library(loo)
loglik1 <- extract(wells1)[["log_lik"]]
loglik2 <- extract(wells2)[["log_lik"]]
loo1 <- loo(loglik1, save_psis = TRUE)
loo2 <- loo(loglik2, save_psis = TRUE)
loo1
loo2
```
```{r}
loo_compare(loo1, loo2)
```


Model 2 is better because it has a higher elpd_loo.

## Question 3e

```{r}
loo1_i <- loo1$pointwise[,1]
loo2_i <- loo2$pointwise[,1]
loo_i_df <- data.frame(loo1_i, loo2_i, y = as.factor(wells$switch))
loo_i_df %>% ggplot(aes(x=loo1_i, y=loo2_i, color=y)) + geom_point() 
```
```{r}
loo_i_diff <- loo2_i - loo1_i
loo_i_df <- data.frame(loo_i_diff = loo_i_diff, 
                       log_arsenic = log(wells$arsenic), 
                       y=factor(wells$switch))
loo_i_df %>% ggplot(aes(x=loo_i_diff, y=log_arsenic, color = y)) + geom_point()
```
  
If the difference between $ELPD_i$ for model 2 and model 1 is positive, Model 2 is better. Model 2 is better for households who did not switch wells and had extremely high arsenic or extremely low arsenic.  Model 2 is better for households who switched wells and had arsenic level between 0 and 1. 
  
## Question 3f

$exp(ELPD_i )$ is the probability of switching wells for house i.

## Question 3g

```{r}
yi_hat_1 <- exp(loo1_i)
yi_hat_2 <- exp(loo2_i)
resid_1 <- wells$switch - yi_hat_1
resid_2 <- wells$switch - yi_hat_2
log_arsenic <- log(wells$arsenic)
log_arsenic_binned <- cut(log_arsenic, breaks = 40)
arsenic_binned <- cut(wells$arsenic, breaks = 40)
```

```{r}
dataframe1 <- data.frame(resid_1, arsenic_binned)
dataframe1 %>% group_by(arsenic_binned) %>%
  summarize(mean_resid_ars_group =  mean(resid_1), se_resid_ars_group = sd(resid_1)/sqrt(length(resid_1))) -> newdataframe1

newdataframe1 %>% ggplot(aes(x=arsenic_binned, y=mean_resid_ars_group)) + geom_point() + geom_errorbar(aes(ymin = mean_resid_ars_group - 2*se_resid_ars_group, ymax = mean_resid_ars_group + 2*se_resid_ars_group)) + labs(x="Arsenic level", y="Mean residual by arsenic group")+ theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```

If the mean residual for a certain arsenic level range is close to 0, the model is predicting well. 

```{r}
dataframe2 <- data.frame(resid_2, log_arsenic_binned)
dataframe2 %>% group_by(log_arsenic_binned) %>%
  summarize(mean_resid_logars_group =  mean(resid_2), se_resid_logars_group = sd(resid_2)/sqrt(length(resid_2))) -> newdataframe1

newdataframe1 %>% ggplot(aes(x=log_arsenic_binned, y=mean_resid_logars_group)) + geom_point() + geom_errorbar(aes(ymin = mean_resid_logars_group - 2*se_resid_logars_group, ymax = mean_resid_logars_group + 2*se_resid_logars_group)) + labs(x="Log Arsenic level", y="Mean residual by log arsenic group") + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
```